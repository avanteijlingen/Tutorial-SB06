{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7421122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas, sys\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0283d6",
   "metadata": {},
   "source": [
    "Load data in, we will be selecting aggregation propensities measured using the Martini 2.1 forcefield for dipeptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6911a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the parameters we just created with the Judred script\n",
    "parameters = pandas.read_csv(\"Dipeptides_Judred.csv\", index_col=0)\n",
    "\n",
    "#Load the AP scores which are stored in APs.csv\n",
    "targets = pandas.read_csv(\"APs.csv\", index_col = 0)\n",
    "\n",
    "#This csv contains more than the data we are looking to match to these specific parameters so firstly,\n",
    "#we filter for just the AP score for Martini 2.1\n",
    "Forcefield = \"2.1\"\n",
    "targets = targets[targets[\"FF\"] == Forcefield]\n",
    "#We want this dataframe to have the same index labels as our parameters, currently parameters.index = [\"AA\", \"AC\", ...]\n",
    "#while targets.index = [\"AA_2.1\", \"AC_2.1\", ...] so we will change it to the 'pep' column which sets targets.index = [\"AA\", \"AC\", ...]\n",
    "targets.index = targets[\"pep\"]\n",
    "\n",
    "#Finally we will take the mean value from the AP scores measure over triplicate runs\n",
    "targets = targets[\"mean\"]\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e5426",
   "metadata": {},
   "source": [
    "We split these into training, testing and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(parameters, targets, test_size=0.33, random_state=9876, shuffle=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=9876, shuffle=True)\n",
    "\n",
    "useRBF = False\n",
    "\n",
    "print(\"Training data:\", X_train.shape)\n",
    "print(\"Testing data:\", X_test.shape)\n",
    "print(\"Validation data:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ec8dc",
   "metadata": {},
   "source": [
    "We will take a look at how projecting our data into other spaces could improve our predictions. But we will uncomment this at the end to determine its effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e46682",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"\"\"\n",
    "X_train, X_val, y_train, y_val = train_test_split(parameters, targets, test_size=0.33, random_state=9876, shuffle=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=9876, shuffle=True)\n",
    "\n",
    "# Add Feature Transformation\n",
    "def rbf(X, epsilon):\n",
    "    return np.e ** -(epsilon*X)**2\n",
    "\n",
    "X_train_RBF = np.hstack((X_train.values, rbf(X_train, 0.2)))\n",
    "X_test_RBF = np.hstack((X_test.values, rbf(X_test, 0.2)))\n",
    "X_val_RBF = np.hstack((X_val.values, rbf(X_val, 0.2)))\n",
    "\n",
    "y_train_RBF = y_train\n",
    "y_test_RBF = y_test\n",
    "y_val_RBF = y_val\n",
    "print(\"Training data:\", X_train_RBF.shape)\n",
    "print(\"Testing data:\", X_test_RBF.shape)\n",
    "print(\"Validation data:\", X_val_RBF.shape)\n",
    "\n",
    "useRBF = True\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3748e4b",
   "metadata": {},
   "source": [
    "Test all the combinations of the hyperparameters on the test set to see what works best for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "BestHyperparameters = pandas.DataFrame(columns = [\"fit_intercept\", \"positive\", \"Test data RMSE\"])\n",
    "iteration = 0\n",
    "for fit_intercept in [True, False]:\n",
    "    for positive in [True, False]:\n",
    "        Hyperparameters = {\"fit_intercept\": fit_intercept,\n",
    "                           \"positive\": positive}\n",
    "        \n",
    "        model = LinearRegression(**Hyperparameters)\n",
    "        \n",
    "        \n",
    "        if useRBF:\n",
    "            #Train on the training data\n",
    "            model.fit(X_train_RBF, y_train_RBF)\n",
    "            predictions = model.predict(X_test_RBF)\n",
    "        else:\n",
    "            #Train on the training data\n",
    "            model.fit(X_train, y_train)        \n",
    "            # Test on the Test data\n",
    "            predictions = model.predict(X_test)\n",
    "        \n",
    "        BestHyperparameters.loc[iteration] = [fit_intercept, positive, mean_squared_error(y_test, predictions, squared=False)]\n",
    "        iteration +=1\n",
    "BestHyperparameters = BestHyperparameters.sort_values(\"Test data RMSE\", ascending=False)\n",
    "print(BestHyperparameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdcdda9",
   "metadata": {},
   "source": [
    "Take our best hyperparameters and make a prediction on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f328b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if useRBF:\n",
    "    try:\n",
    "        epsilon = BestHyperparameters.iloc[-1][\"epsilon\"]\n",
    "    except KeyError:\n",
    "        print(\"You have not added an 'epsilon' column to the BestHyperparameters dataframe! Reverting to 0.2\")\n",
    "        epsilon = 0.2\n",
    "        X_train = np.hstack((X_train.values, rbf(X_train, epsilon)))\n",
    "        X_test = np.hstack((X_test.values, rbf(X_test, epsilon)))\n",
    "        X_val = np.hstack((X_val.values, rbf(X_val, epsilon)))\n",
    "\n",
    "    \n",
    "\n",
    "Hyperparameters = {\"fit_intercept\": BestHyperparameters.iloc[-1][\"fit_intercept\"],\n",
    "                   \"positive\": BestHyperparameters.iloc[-1][\"positive\"]}\n",
    "model = LinearRegression(**Hyperparameters)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Validate on the never before seen in any way validation data\n",
    "predictions = model.predict(X_val)\n",
    "RMSE = mean_squared_error(y_val, predictions, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8446688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(predictions, y_val)\n",
    "plt.plot([1,2.7], [1,2.7], lw=1, c=\"black\")\n",
    "plt.title(f\"RMSE = \" + \"{:1.3f}\".format(RMSE))\n",
    "plt.xlabel(\"Predicted AP\")\n",
    "plt.ylabel(\"True AP\")\n",
    "plt.gcf().set_dpi(150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e98e760",
   "metadata": {},
   "source": [
    "# Now, go back and add the kernel trick to the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
